{
  "Design a news feed system": {
    "Requirements Analysis & Scoping": "- Functional Requirements: User can post updates, see a news feed of posts from people they follow, and like/comment on posts.\n- Non-Functional Requirements: High availability, low latency for feed generation, scalability to millions of users.\n- Out of Scope: Real-time chat, complex content ranking algorithms (initially).",
    "High-Level Architecture": "- Web Server: Handles incoming user requests.\n- Application Server: Contains the core business logic.\n- Database: Stores user data, posts, and social graph.\n- Cache: Caches news feeds for fast retrieval.\n- Load Balancer: Distributes traffic across multiple servers.",
    "Component Deep-Dive": "- User Service: Manages user accounts and authentication.\n- Post Service: Handles creating, reading, and updating posts.\n- Feed Generation Service: Generates news feeds for users. This can be a push or pull model.\n- Fanout Service (for push model): Pushes new posts to the feeds of followers.",
    "Scalability and Bottleneck Analysis": "- Database Scaling: Use read replicas to handle read-heavy workloads. Shard the database by user ID.\n- Caching: Cache user feeds in Redis or a similar in-memory store.\n- Feed Generation: For users with many followers (celebrities), feed generation can be a hybrid push/pull model.\n- Load Balancing: Use a load balancer to distribute traffic and prevent single points of failure."
  },
  "Design a URL shortening service like TinyURL": {
    "Requirements Analysis & Scoping": "**Functional Requirements:**\n*   **Shorten URL:** Given a long URL, the service must generate and return a much shorter, unique URL.\n*   **Redirect:** When a user accesses a short URL, they must be redirected to the original long URL.\n*   **Custom Alias (Optional):** Allow users to specify a custom short name for their URL, if available.\n*   **Link Expiration (Optional):** Links should expire after a configurable amount of time.\n\n**Non-Functional Requirements:**\n*   **High Availability:** The service must be highly available.\n*   **Low Latency:** Redirects must be extremely fast.\n*   **Scalability:** The system must be able to handle a massive number of URLs and a very high rate of redirect requests.\n*   **Uniqueness:** Shortened URLs must be unique and cannot be easily predicted.",
    "High-Level Architecture": "**High-Level Design & API**\n\nWe can design a simple REST API for the service.\n\n*   **`POST /api/v1/shorten`**: Creates a new short URL.\n*   **`GET /{shortCode}`**: Redirects to the original URL.",
    "Component Deep-Dive": "**Data Model and Database Choice**\n\nThe core of our service is a key-value mapping where the key is the `shortCode` and the value is the `longUrl`. A **NoSQL Key-Value store** like DynamoDB or Cassandra is ideal due to its scalability and performance for key-based lookups.\n\n**Core Logic: Generating the `shortCode`**\n\nThe recommended approach is **Base-62 conversion**.\n1.  Use a distributed counter (ID Generator Service) to get a unique integer ID for each new URL.\n2.  Convert this integer ID to a Base-62 string (`[a-zA-Z0-9]`).\n*   This guarantees uniqueness and a 7-character string can represent over 3.5 trillion URLs.",
    "Scalability and Bottleneck Analysis": "**Detailed System Architecture**\n\n1.  **Client** sends request to a **Load Balancer**.\n2.  **Load Balancer** routes to a stateless **Application Service**.\n3.  **Write Path (`POST /shorten`):**\n    1.  Application Service gets a unique ID from the ID Generator Service.\n    2.  Converts ID to a Base-62 `shortCode`\n    3.  Saves (`shortCode`, `longUrl`) to the NoSQL Database.\n4.  **Read Path (`GET /{shortCode}`):**\n    1.  Application Service checks a **Cache** (like Redis) first.\n    2.  **Cache Hit:** Returns 301 redirect immediately.\n    3.  **Cache Miss:** Queries the NoSQL Database, populates the cache, and then returns the 301 redirect.\n\n**Further Optimizations**\n\n*   **Analytics:** Use a message queue (like Kafka) to process click data asynchronously.\n*   **Security:** Implement rate limiting and malicious URL filtering.\n*   **Vanity URLs:** Allow users to pick custom `shortCode`s if they are available."
  },
  "Design a video streaming service like YouTube or Netflix": {
    "Requirements Analysis & Scoping": "**Core Features:**\n*   **Video Upload:** Users can upload raw video files.\n*   **Video Processing (Transcoding):** Videos are converted into multiple formats and resolutions for Adaptive Bitrate Streaming.\n*   **Global Content Delivery:** Processed videos are distributed across the globe for low-latency streaming.\n*   **Video Playback:** Users can stream videos on demand.",
    "High-Level Architecture": "**Upload Flow:**\n1.  User uploads video to API Gateway, which routes to a Video Ingestion Service.\n2.  The service uploads the raw video to an Object Storage (like S3) and sends a message to a Message Queue (like SQS) with video details.\n3.  Video Processing Workers (e.g., Lambda) are triggered by the message queue.\n4.  Workers transcode the video into multiple formats (Adaptive Bitrate Streaming), generate thumbnails, and create a manifest file (HLS/DASH).\n5.  Processed files are stored in another Object Storage bucket.\n\n**Playback Flow:**\n1.  User's client requests video from the application.\n2.  The application fetches the video metadata and the manifest URL from the database.\n3.  The video player uses the manifest to request video segments from a **Content Delivery Network (CDN)**.\n4.  The CDN serves the video segments from the edge location closest to the user, ensuring low latency.\n5.  If the content is not in the CDN cache (cache miss), it is fetched from the processed video Object Storage.",
    "Component Deep-Dive": "**Core Components**\n*   **API Gateway:** Manages and routes API requests.\n*   **Video Ingestion Service:** Handles the initial video upload.\n*   **Object Storage (S3):** Stores raw and processed video files.\n*   **Message Queue (SQS):** Decouples the upload and processing workflows, providing reliability.\n*   **Video Processing Workers (Lambda/FFmpeg):** Perform transcoding and other processing tasks.\n*   **Database (PostgreSQL/Elasticsearch):** Stores metadata and enables search.",
    "Scalability and Bottleneck Analysis": "**CDN (CloudFront):** Caches and delivers video content globally for fast playback."
  },
  "Design a ride-sharing service like Uber or Lyft": {
    "Requirements Analysis & Scoping": "**Core Functional Requirements**\n\n*   **Riders:** Request rides, see ETA/fare, track drivers, pay, and rate.\n*   **Drivers:** Set availability, accept/reject rides, navigate, and rate riders.\n*   **System:** Match riders and drivers, calculate fares, process payments.\n\n**Non-Functional Requirements**\n\n*   **High Availability:** Service must be 24/7 operational.\n*   **Low Latency:** Real-time location updates and matching must be fast.\n*   **Scalability:** Handle millions of users and concurrent rides.",
    "High-Level Architecture": "**High-Level Architecture (Microservices)**\n\n*   **API Gateway:** Single entry point for all clients.\n*   **User Service:** Manages user profiles and authentication.\n*   **Driver/Location Service:** Tracks real-time driver locations using a geospatial index (like Redis with Geo-hashing).\n*   **Trip Service:** Manages the state of a ride (requested, accepted, in-progress, completed).\n*   **Matching Service:** Finds the best driver for a ride request.\n*   **Payment Service:** Handles fare calculation and payment processing.\n*   **Notification Service:** Sends push notifications/SMS.\n*   **Message Queue (Kafka/RabbitMQ):** Decouples services for resilience.",
    "Component Deep-Dive": "**Data Schema**\n\n*   **Relational Database (PostgreSQL):** For transactional data like users, rides, and payments.\n*   **NoSQL/Cache (Redis):** For high-throughput data like real-time driver locations.",
    "Scalability and Bottleneck Analysis": "**Core Workflow: Matching a Rider with a Driver**\n\n1.  Driver's app continuously sends location updates to the **Location Service**.\n2.  Rider requests a ride via the **Trip Service**.\n3.  The **Matching Service** queries the **Location Service** for nearby available drivers (using geohashing for efficiency).\n4.  The **Matching Service** selects the best driver and sends them a notification.\n5.  If the driver accepts, the **Trip Service** updates the ride status and notifies both parties. If not, the process repeats with the next best driver."
  },
  "Design a search engine like Google": {
    "Requirements Analysis & Scoping": "A search engine has two main parts:\n*   **Offline Processing (Crawling & Indexing):** Discovers, fetches, and organizes web pages into a searchable data structure.\n*   **Online Processing (Querying):** Handles live user queries, finds relevant documents, ranks them, and returns results.",
    "High-Level Architecture": "**Web Crawler:** Discovers and downloads web pages. It starts with a seed set of URLs and follows links to find new pages.\n*   **Indexer & Inverted Index:** This is the core of search. The indexer creates an **Inverted Index**, which maps words to a list of documents containing that word. This allows for fast lookups. This process is often done using a framework like MapReduce.",
    "Component Deep-Dive": "**Query Engine:** Handles user queries.\n    1.  **Query Processor:** Parses the user's query and finds matching documents by looking up terms in the Inverted Index.\n    2.  **Ranker:** Scores the matching documents based on hundreds of signals (e.g., **PageRank**, TF-IDF, freshness, user data) to determine relevance.\n    3.  **Aggregator:** Formats the ranked results into a user-friendly page.",
    "Scalability and Bottleneck Analysis": "**Inverted Index Sharding:** The index is partitioned (sharded) across thousands of servers. A query is sent to all shards in parallel.\n*   **Replication:** Each shard is replicated to ensure high availability.\n*   **Ranking (PageRank):** PageRank is a key algorithm that measures a page's authority based on the quantity and quality of links pointing to it. It's calculated offline.\n*   **Low Latency:** Achieved through heavy caching, in-memory index storage, and massive parallelization."
  },
  "Design a distributed file storage system like Google Drive or Dropbox": {
    "Requirements Analysis & Scoping": "**Core Concepts**\n\n*   **File Chunking:** Files are broken into smaller, fixed-size blocks (chunks). This allows for efficient handling of large files, data deduplication, and bandwidth savings, as only changed chunks need to be synced.\n*   **Metadata:** Information about files (names, sizes, versions, folder structure) is stored separately from the file content itself.\n*   **Synchronization:** The process of keeping files consistent across multiple devices and the cloud.",
    "High-Level Architecture": "**Client:** The application on the user's device that monitors for file changes and communicates with the backend.\n*   **API Gateway:** A single entry point for all client requests, handling authentication and routing.\n*   **Metadata Service:** Manages file metadata, backed by a relational database (e.g., PostgreSQL).\n*   **Synchronization Service:** Coordinates the file sync process.\n*   **Block Storage Service:** Manages the actual file chunks.\n*   **Object/Block Storage (S3):** The physical storage for the file chunks.\n*   **Notification Service:** Pushes real-time updates to clients about changes.\n*   **Message Queue:** Decouples services for asynchronous tasks.",
    "Component Deep-Dive": "**Synchronization Workflow**\n\n1.  The client detects a file change and notifies the **Synchronization Service**.\n2.  The client sends a list of chunk hashes for the changed file.\n3.  The backend checks which chunks it already has (deduplication).\n4.  The client uploads only the new, unique chunks.\n5.  The **Metadata Service** is updated to reflect the new file version.\n6.  The **Notification Service** informs other clients of the change, and they download the updated chunks.",
    "Scalability and Bottleneck Analysis": "**Metadata Bottlenecks:** The metadata service is a common bottleneck. As the number of files and users grows, a single metadata server can be overwhelmed. This can be mitigated by sharding the metadata based on user ID or other criteria.\n**Data Path Bottlenecks:** Network saturation and disk I/O on storage nodes can become bottlenecks. This can be addressed by using content delivery networks (CDNs) to cache frequently accessed files and by distributing data across multiple storage nodes."
  },
  "Design a chat application like WhatsApp or Facebook Messenger": {
    "Requirements Analysis & Scoping": "**Core Features**\n\n*   **1-to-1 & Group Messaging:** Real-time message exchange.\n*   **Message Status:** Sent, delivered, and read receipts.\n*   **User Presence:** Online, offline, and typing indicators.\n*   **End-to-End Encryption (E2EE):** Secure communication using a protocol like Signal.",
    "High-Level Architecture": "**Clients (iOS, Android, Web):** Connect to the backend using **WebSockets** for persistent, real-time communication.\n*   **WebSocket Gateway:** A stateful service that manages client connections and routes messages.\n*   **Backend Microservices:**\n    *   **Auth & User Services:** Handle user accounts, profiles, and contacts.\n    *   **Message Service:** Persists messages and handles message delivery logic.\n    *   **Presence Service:** Manages user online/offline status.",
    "Component Deep-Dive": "**Databases:**\n    *   **NoSQL (Cassandra):** For storing the massive volume of chat messages, partitioned by chat ID.\n    *   **SQL (PostgreSQL):** For user accounts and relational data.\n    *   **Cache (Redis):** For ephemeral data like user presence and connection information.",
    "Scalability and Bottleneck Analysis": "**Message Flow (User A to User B)**\n\n1.  Client A sends an encrypted message to its **WebSocket Gateway**.\n2.  The Gateway forwards the message to the **Message Service**.\n3.  The Message Service stores the message in **Cassandra**.\n4.  The Message Service checks User B's status in **Redis**.\n5.  If User B is online, the message is pushed to their Gateway and then to their client. If offline, a push notification is sent."
  },
  "Design a stock trading platform": {
    "Requirements Analysis & Scoping": "**Core Principles**\n\n*   **Security First:** Protect user data, funds, and transaction integrity.\n*   **Reliability & Availability:** High uptime, especially during market hours.\n*   **Low Latency:** Real-time data and swift order execution.\n*   **Compliance:** All transactions must be logged and auditable.",
    "High-Level Architecture": "**API Gateway:** Single, secure entry point for all client requests.\n*   **User Service:** Manages identity, authentication, and KYC.\n*   **Ledger Service:** The immutable source of truth for all financial data (cash, holdings). This is the most critical service.\n*   **Trading Service:** Manages the lifecycle of orders, from validation to execution routing.\n*   **Market Data Service:** Ingests and distributes real-time market data via WebSockets.\n*   **Portfolio Service:** Calculates and serves aggregated portfolio data.\n*   **Asynchronous Communication (Kafka):** A message bus to decouple services and handle events like trade executions asynchronously.",
    "Component Deep-Dive": "**Data Management**\n\n*   **Transactional DB (PostgreSQL):** For services requiring ACID compliance (Ledger, Trading, User).\n*   **Time-Series DB (TimescaleDB/InfluxDB):** For historical market data.\n*   **In-Memory Cache (Redis):** For caching real-time prices and user sessions.",
    "Scalability and Bottleneck Analysis": "**Key System Flow: Placing an Order**\n\n1.  A client sends an order to the **Trading Service** via the **API Gateway**.\n2.  The **Trading Service** validates the order and checks for sufficient funds/shares with the **Ledger Service**.\n3.  The order is routed to a third-party **Execution Broker**.\n4.  Upon execution, the broker confirms with the **Trading Service**.\n5.  The **Trading Service** publishes a `TRADE_EXECUTED` event to **Kafka**.\n6.  The **Ledger Service**, **Portfolio Service**, and **Notification Service** consume this event to update their respective states, ensuring data consistency across the platform."
  },
  "Design an e-commerce website like Amazon": {
    "Requirements Analysis & Scoping": "**Core Principle: Microservices Architecture**\n\n*   The system is built as a collection of small, independent services. This is essential for scalability, independent development, and fault tolerance.",
    "High-Level Architecture": "**Microservice Breakdown**\n\n*   **User Service:** Manages user registration, login, profiles, and addresses.\n*   **Product Service:** Manages the product catalog, including descriptions, images, and prices.\n*   **Search Service:** Powers fast product search using a dedicated search engine like Elasticsearch.\n*   **Inventory Service:** Manages stock levels for each product.\n*   **Cart Service:** Manages shopping carts, often using a fast in-memory database like Redis.\n*   **Order Service:** Coordinates the checkout process, interacting with other services.\n*   **Payment Service:** Integrates with third-party payment providers.",
    "Component Deep-Dive": "**Key User Flow: Placing an Order**\n\n1.  A user initiates checkout, and the request is routed to the **Order Service**.\n2.  The **Order Service** begins a transaction:\n    a.  It calls the **Inventory Service** to reserve the items.\n    b.  It calls the **Payment Service** to process the payment.\n    c.  If payment is successful, it saves the order to its database.\n    d.  It publishes an `OrderCreated` event to a message broker (like RabbitMQ).\n3.  Other services (like a Notification Service) can then react to this event to send a confirmation email.",
    "Scalability and Bottleneck Analysis": "**Database Bottlenecks:** The database is a common bottleneck. This can be addressed with read replicas, sharding, and using different database types for different needs (polyglot persistence).\n**Caching:** A multi-layered caching strategy (CDN, in-memory cache, application-level cache) is crucial for performance.\n**Third-Party Services:** External services like payment gateways can be a bottleneck. Use circuit breakers and asynchronous calls to mitigate this."
  },
  "Design a web crawler": {
    "Requirements Analysis & Scoping": "**Key Design Considerations**\n\n*   **Scalability:** The system must be distributed to handle the vastness of the web.\n*   **Robustness:** The crawler must be resilient to issues like malformed HTML and unresponsive servers.\n*   **Politeness:** To avoid overloading servers, the crawler must respect `robots.txt` and limit the request rate to any single server.",
    "High-Level Architecture": "**Core Components**\n\n*   **URL Frontier:** Manages the list of URLs to be crawled, acting as a prioritized queue. It's initialized with a set of seed URLs.\n*   **Fetcher/Downloader:** Downloads web pages for the URLs provided by the URL Frontier.\n*   **HTML Parser:** Extracts links and other data from the downloaded pages.\n*   **Duplicate Detection:** Avoids processing the same content multiple times by storing and checking content fingerprints (e.g., hashes).\n*   **Data Storage:** Stores the crawled data (e.g., in a distributed file system like HDFS or a NoSQL database).",
    "Component Deep-Dive": "**Crawling Process**\n\n1.  The process starts with seed URLs in the **URL Frontier**.\n2.  The **Fetcher** gets a URL, resolves its DNS, and downloads the page.\n3.  The **Parser** extracts links from the page.\n4.  The content is checked for duplicates. If it's new, it's stored.\n5.  Extracted links are filtered and added to the **URL Frontier** for future crawling.",
    "Scalability and Bottleneck Analysis": "**Network I/O:** The crawler is network-bound. This can be addressed by using a distributed crawling approach with multiple crawlers working in parallel.\n**CPU Processing:** Parsing HTML and extracting links can be CPU-intensive. This can be scaled by adding more processing nodes.\n**Memory and Storage:** The URL frontier and visited set can consume large amounts of memory. Use efficient data structures like Bloom filters for the visited set and a distributed queue for the frontier."
  },
  "Design a recommendation system": {
    "Requirements Analysis & Scoping": "**Functional Requirements:**\n*   Recommend personalized items (e.g., products, movies, articles) to users.\n*   Handle different recommendation contexts (e.g., 'you might also like', 'trending now').\n*   Collect user interaction data (clicks, purchases, ratings) to improve recommendations.\n\n**Non-Functional Requirements:**\n*   **Scalability:** Handle millions of users and items.\n*   **Low Latency:** Generate recommendations quickly for a real-time user experience.\n*   **Flexibility:** Allow for easy experimentation with different recommendation algorithms.",
    "High-Level Architecture": "**Two Main Stages:**\n1.  **Offline Candidate Generation:** Periodically, a batch processing system (like Spark) analyzes all user interaction data to generate a large set of potential recommendations for each user. This is computationally expensive.\n    *   **Input:** User history, item metadata.\n    *   **Output:** A list of (user_id, [item_id_1, item_id_2, ...]) stored in a cache or NoSQL database.\n2.  **Online Ranking/Filtering:** When a user requests recommendations, a lightweight service fetches the pre-computed candidates, applies real-time filtering (e.g., remove already purchased items), and ranks them using a simpler model before returning the final list.",
    "Component Deep-Dive": "**Core Components**\n*   **Data Collection Service:** Ingests user events (clicks, views, purchases) and sends them to a message queue (like Kafka).\n*   **Data Lake/Warehouse (S3/BigQuery):** Stores raw event data for batch processing.\n*   **Batch Processing (Spark/Hadoop):** Runs recommendation algorithms (e.g., Collaborative Filtering, Matrix Factorization) on the raw data to generate candidate sets.\n*   **Candidate Store (Redis/DynamoDB):** Stores the pre-computed recommendations for fast retrieval.\n*   **Recommendation Service (API):** The real-time service that fetches candidates, performs final ranking/filtering, and serves the recommendations to the user.",
    "Scalability and Bottleneck Analysis": "**Candidate Generation:** The offline batch job is a bottleneck. It can be scaled by using a distributed processing framework like Spark and optimizing the algorithms.\n**Real-time Serving:** The Recommendation Service must have low latency. This is achieved by pre-computing candidates offline. The service itself is stateless and can be horizontally scaled.\n**Feedback Loop:** A critical aspect is the feedback loop. User interactions with recommendations must be fed back into the system to retrain models and improve future recommendations. This is often handled via the asynchronous data collection pipeline."
  }
}